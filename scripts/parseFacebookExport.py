#!/usr/bin/env python3
"""
Facebook Export HTML ã‚’è§£æã—ã¦ NotebookLM ç”¨ Markdown ã‚’ç”Ÿæˆ
"""

import re
import json
from pathlib import Path
from datetime import datetime
from html import unescape

# ãƒ‘ã‚¹è¨­å®š
EXPORT_PATH = Path("/Users/takasaki19841121/Desktop/ifJukuManager/facebook-shotatakasaki37-2026_01_19-XwtXfVf2")
POSTS_HTML = EXPORT_PATH / "your_facebook_activity" / "posts" / "your_posts__check_ins__photos_and_videos_1.html"
OUTPUT_DIR = Path(__file__).parent.parent / "data" / "social"
JSON_OUTPUT = OUTPUT_DIR / "facebook_posts.json"
MD_OUTPUT = OUTPUT_DIR / "facebook_posts_for_notebooklm.md"


def clean_text(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    text = re.sub(r'<br\s*/?>', '\n', text)
    text = re.sub(r'<a[^>]*>([^<]*)</a>', r'\1', text)
    text = re.sub(r'<[^>]+>', '', text)
    text = unescape(text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    return text.strip()


def is_meaningful_text(text):
    """æœ‰ç”¨ãªãƒ†ã‚­ã‚¹ãƒˆã‹ã©ã†ã‹åˆ¤å®š"""
    if not text or len(text) < 30:
        return False
    if 'ã«æ›´æ–°' in text and len(text) < 50:
        return False
    if text.startswith('http') and len(text) < 100:
        return False
    if 'é«˜å´ ç¿”å¤ªã•ã‚“ãŒ' in text:
        return False
    if text.startswith('å ´æ‰€:') and len(text) < 100:
        return False
    return True


def extract_date_for_text(html_content, text_start_pos):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ä½ç½®ã‹ã‚‰æœ€ã‚‚è¿‘ã„æ—¥ä»˜ã‚’å–å¾—"""
    # ãƒ†ã‚­ã‚¹ãƒˆã®å‰ã«ã‚ã‚‹æœ€ã‚‚è¿‘ã„æ—¥ä»˜ã‚’æ¢ã™
    search_area = html_content[:text_start_pos]
    dates = re.findall(r'<div class="_a72d">([^<]+)</div>', search_area)
    if dates:
        return dates[-1]  # æœ€å¾Œã«è¦‹ã¤ã‹ã£ãŸæ—¥ä»˜ï¼ˆæœ€ã‚‚è¿‘ã„ï¼‰
    return None


def parse_posts():
    """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦æŠ•ç¨¿ã‚’æŠ½å‡º"""
    print(f"ğŸ“– èª­ã¿è¾¼ã¿ä¸­: {POSTS_HTML}")

    with open(POSTS_HTML, "r", encoding="utf-8") as f:
        html_content = f.read()

    posts = []
    seen_texts = set()

    # æ—¥æœ¬èªã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
    # ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã‚’å«ã‚€div
    pattern = r'<div>([^<]*[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥][^<]*(?:<br\s*/?>|<a[^>]*>[^<]*</a>)*[^<]*)</div>'

    for match in re.finditer(pattern, html_content, re.DOTALL):
        raw_text = match.group(1)
        text = clean_text(raw_text)

        if is_meaningful_text(text):
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            text_key = text[:80]
            if text_key not in seen_texts:
                seen_texts.add(text_key)

                # æ—¥ä»˜ã‚’å–å¾—
                date = extract_date_for_text(html_content, match.start())

                posts.append({
                    "date": date,
                    "text": text,
                    "has_content": True
                })

    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    def parse_date(date_str):
        if not date_str:
            return datetime.min
        try:
            # "1æœˆ 28, 2025 1:47:43 PM" å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
            months = {'1æœˆ': 1, '2æœˆ': 2, '3æœˆ': 3, '4æœˆ': 4, '5æœˆ': 5, '6æœˆ': 6,
                     '7æœˆ': 7, '8æœˆ': 8, '9æœˆ': 9, '10æœˆ': 10, '11æœˆ': 11, '12æœˆ': 12}
            for jp_month, num in months.items():
                if jp_month in date_str:
                    date_str = date_str.replace(jp_month, f"{num}æœˆ")
            match = re.match(r'(\d+)æœˆ\s*(\d+),\s*(\d+)', date_str)
            if match:
                m, d, y = match.groups()
                return datetime(int(y), int(m), int(d))
        except:
            pass
        return datetime.min

    posts.sort(key=lambda x: parse_date(x.get('date', '')), reverse=True)

    print(f"âœ… {len(posts)} ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ•ç¨¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    return posts


def analyze_writing_style(posts):
    """æ–‡ä½“ã‚’åˆ†æ"""
    if not posts:
        return {"total_posts": 0}

    texts = [p["text"] for p in posts]
    all_text = " ".join(texts)

    # çµµæ–‡å­—æŠ½å‡º
    emoji_pattern = re.compile(
        "["
        "\U0001F300-\U0001F9FF"
        "\U00002600-\U000026FF"
        "\U00002700-\U000027BF"
        "âœ¨ğŸ‘ğŸ»"
        "]+",
        flags=re.UNICODE
    )
    emojis = emoji_pattern.findall(all_text)
    emoji_count = {}
    for e in emojis:
        for char in e:
            emoji_count[char] = emoji_count.get(char, 0) + 1
    top_emojis = sorted(emoji_count.items(), key=lambda x: -x[1])[:10]

    # ã‚ˆãä½¿ã†è¡¨ç¾
    expressions = []
    exp_patterns = [
        ('w', 'wï¼ˆç¬‘ã„ï¼‰'),
        ('ï¼ï¼', 'ï¼ï¼ï¼ˆå¼·èª¿ï¼‰'),
        ('ã€œ', 'ã€œï¼ˆèªå°¾ä¼¸ã°ã—ï¼‰'),
        ('ã‚ˆã‚ã—ã', 'ã‚ˆã‚ã—ã'),
        ('ãƒ©ã‚¤ãƒ–', 'ãƒ©ã‚¤ãƒ–é…ä¿¡'),
        ('AI', 'AIé–¢é€£'),
    ]
    for pattern, desc in exp_patterns:
        count = all_text.count(pattern)
        if count >= 2:
            expressions.append(f"{desc}({count}å›)")

    return {
        "total_posts": len(posts),
        "average_length": sum(len(t) for t in texts) // len(texts) if texts else 0,
        "max_length": max(len(t) for t in texts) if texts else 0,
        "top_emojis": [e[0] for e in top_emojis],
        "expressions": expressions,
    }


def save_results(posts):
    """çµæœã‚’ä¿å­˜"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    style = analyze_writing_style(posts)

    # JSONä¿å­˜
    archive = {
        "fetched_at": datetime.now().isoformat(),
        "user_name": "é«˜å´ç¿”å¤ª",
        "source": "Facebook Export",
        "total_posts": len(posts),
        "posts": posts,
        "writing_style": style,
    }

    with open(JSON_OUTPUT, "w", encoding="utf-8") as f:
        json.dump(archive, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ JSONä¿å­˜: {JSON_OUTPUT}")

    # Markdownä¿å­˜
    md_content = f"""# é«˜å´ç¿”å¤ªã®FacebookæŠ•ç¨¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–

## æ¦‚è¦

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯é«˜å´ç¿”å¤ªï¼ˆifå¡¾ å¡¾é ­ï¼‰ã®FacebookæŠ•ç¨¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
ã“ã‚Œã‚‰ã®æŠ•ç¨¿ã‚’å‚è€ƒã«ã—ã¦ã€é«˜å´ã•ã‚“ã®æ–‡ä½“ãƒ»å£èª¿ã‚’çœŸä¼¼ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

## æ–‡ä½“ã®ç‰¹å¾´

- **æŠ•ç¨¿æ•°**: {style.get('total_posts', 0)}ä»¶
- **å¹³å‡æ–‡å­—æ•°**: {style.get('average_length', 0)}æ–‡å­—
- **æœ€å¤§æ–‡å­—æ•°**: {style.get('max_length', 0)}æ–‡å­—
- **ã‚ˆãä½¿ã†çµµæ–‡å­—**: {' '.join(style.get('top_emojis', []))}
- **ã‚ˆãä½¿ã†è¡¨ç¾**: {', '.join(style.get('expressions', []))}

## é«˜å´ã•ã‚“ã®æ–‡ä½“ã®ãƒã‚¤ãƒ³ãƒˆ

ä»¥ä¸‹ã®ç‰¹å¾´ã‚’çœŸä¼¼ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

1. **ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå£èª¿**
   - ã€Œã€œã§ã”ã–ã‚“ã™ã€ã€Œã€œãªã‚“ã ãŒã€ãªã©ç‹¬ç‰¹ã®è¨€ã„å›ã—
   - ã€Œwã€ã‚’ç¬‘ã„ã¨ã—ã¦ä½¿ç”¨
   - ã€Œï¼ï¼ã€ã§å¼·èª¿

2. **å…·ä½“çš„ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§**
   - ã€Œä»Šæ—¥ã¯ã€œã€ã€Œæœ¬æ—¥ã€œã€ã§å§‹ã¾ã‚‹ã“ã¨ãŒå¤šã„
   - å®Ÿéš›ã®ä½“é¨“ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å…±æœ‰

3. **ãƒã‚¸ãƒ†ã‚£ãƒ–ã§è¡Œå‹•çš„**
   - AIã‚„ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã¸ã®ç©æ¥µçš„ãªå§¿å‹¢
   - æ•™è‚²ã¸ã®æƒ…ç†±
   - å­è‚²ã¦ã¨ä»•äº‹ã®ä¸¡ç«‹

4. **å°‚é–€æ€§ã¨è¦ªã—ã¿ã‚„ã™ã•ã®ä¸¡ç«‹**
   - IT/AIé–¢é€£ã®å°‚é–€çš„ãªå†…å®¹
   - ã§ã‚‚é›£ã—ããªã‚‰ãšè¦ªã—ã¿ã‚„ã™ã„è¡¨ç¾

---

## æŠ•ç¨¿ä¸€è¦§

"""

    for i, post in enumerate(posts, 1):
        md_content += f"### {i}. {post.get('date', 'æ—¥ä»˜ä¸æ˜')}\n\n"
        md_content += f"{post['text']}\n\n"
        md_content += "---\n\n"

    with open(MD_OUTPUT, "w", encoding="utf-8") as f:
        f.write(md_content)
    print(f"ğŸ“ Markdownä¿å­˜: {MD_OUTPUT}")
    print(f"\nâœ… NotebookLMã«ã¯ {MD_OUTPUT} ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")


def main():
    if not POSTS_HTML.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {POSTS_HTML}")
        return

    posts = parse_posts()

    if posts:
        print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿:")
        for post in posts[:5]:
            print(f"\n  [{post.get('date', '?')}]")
            text = post['text'][:150] + "..." if len(post['text']) > 150 else post['text']
            print(f"  {text}")

        save_results(posts)
    else:
        print("âŒ æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == "__main__":
    main()
